Friendship Engine
> **Status:** Exploratory research prototype  
> **Focus:** Human‚ÄìAI Interaction, emotional support, conversational policy design  
> **Note:** This is not a chatbot product, but an experimental system built to study how interaction design choices affect perceived emotional bonding with AI.
> 
Exploring Proactive Conversational AI for Emotional Support and Social Bonding

Prototype / research exploration. Not a human.


Overview

Friendship Engine is an exploratory conversational AI project investigating whether interaction design choices‚Äîrather than raw model capability‚Äîcan increase a user‚Äôs perceived sense of being listened to, understood, and emotionally supported.

The project focuses on proactive but constrained curiosity, lightweight memory, and contextual follow-ups to study how humans emotionally respond to AI systems in moments of stress, uncertainty, or isolation.

This is not an attempt to replace human relationships.
It is an investigation into AI as a non-judgmental conversational space.



Research Question

Can a conversational AI that asks carefully constrained, context-sensitive follow-up questions foster a perceived emotional bond and encourage self-disclosure, despite lacking genuine human emotion?



Hypothesis

A conversational agent with:
	‚Ä¢	proactive but limited curiosity,
	‚Ä¢	short-term coherent memory,
	‚Ä¢	emotionally aligned follow-up questions,

will produce:
	‚Ä¢	longer and more engaged conversations,
	‚Ä¢	higher perceived emotional support,
	‚Ä¢	increased willingness to share personal concerns,

compared to a purely reactive conversational style.



Design Principles
	‚Ä¢	Anti-interrogation rule: maximum one question per agent message
	‚Ä¢	Reflection before questioning
	‚Ä¢	Memory limited to non-sensitive context
	‚Ä¢	Explicit disclosure: ‚ÄúPrototype / experiment. Not a human.‚Äù
	‚Ä¢	No dependency-inducing language
	‚Ä¢	Privacy-first: local execution, no cloud logging



Implementation
	‚Ä¢	Language: Python
	‚Ä¢	LLM backend: Local model via Ollama (e.g. LLaMA 3.1 8B)
	‚Ä¢	Architecture:
	‚Ä¢	app.py ‚Äì CLI interaction loop
	‚Ä¢	llm.py ‚Äì model wrapper
	‚Ä¢	policy.py ‚Äì curiosity and follow-up decision logic
	‚Ä¢	memory.py ‚Äì constrained memory handling
	‚Ä¢	metrics.py ‚Äì session metrics & self-report surveys
	‚Ä¢	configs.py ‚Äì A/B mode and experiment configuration

The system runs fully locally and logs anonymized interaction metrics.



Evaluation (Exploratory)

The prototype was tested in five in-person sessions.

Self-reported scores (1‚Äì10): 3, 5, 6, 7, 8


Qualitative feedback themes:
	‚Ä¢	The agent felt attentive and interested
	‚Ä¢	Users appreciated follow-up questions
	‚Ä¢	The system was perceived as non-judgmental
	‚Ä¢	Lack of genuine human warmth was noted
	‚Ä¢	Several users felt more comfortable opening up than with people



Findings
	‚Ä¢	AI does not replicate human emotional bonding
	‚Ä¢	AI can provide functional emotional support by:
	‚Ä¢	lowering barriers to self-expression
	‚Ä¢	sustaining reflective conversation
	‚Ä¢	offering a psychologically safe space

Friendship Engine suggests emotional bonding is an emergent property of system design, not a binary model capability.



Limitations
	‚Ä¢	Small sample size
	‚Ä¢	Qualitative evaluation
	‚Ä¢	Single model and policy configuration

These limitations are intentional: the project prioritizes insight over statistical generalization.

## Ethical Positioning

This project explicitly avoids framing the agent as a replacement for human relationships.

Design constraints include:
- Clear disclosure that the system is a prototype and not a human.
- No use of dependency-inducing language (e.g., exclusivity, guilt, emotional manipulation).
- Memory limited to non-sensitive conversational context.
- Gentle redirection toward human support if strong distress emerges.

The goal is to explore AI as a complementary emotional tool, not as a substitute for human connection.

Future Directions
	‚Ä¢	Controlled A/B studies on curiosity policies
	‚Ä¢	Longitudinal user interaction
	‚Ä¢	Adaptive memory decay
	‚Ä¢	Ethical boundaries in emotionally supportive AI
	‚Ä¢	Extensions toward Human‚ÄìAI Interaction and social robotics



Why This Matters

As AI systems enter increasingly personal domains, understanding how design constraints shape emotional experience becomes critical.

Friendship Engine frames emotional support not as a replacement for human connection, but as a complementary tool‚Äîespecially in moments of loneliness, stress, or fear of judgment.


Status

üß™ Active research prototype (MVP v0.1)
Built for experimentation, iteration, and learning.



This project treats emotional bonding not as a property of the language model alone, but as an emergent outcome of system design, constraints, and interaction policy.
